{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "825dad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from core.environment.gridworlds import GridHardXY, GridHardRGB, GridTwoRoomRGB, GridOneRoomRGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53f60348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from core.utils.torch_utils import random_seed\n",
    "\n",
    "\n",
    "class GridHardXY:\n",
    "    \n",
    "    def __init__(self, seed=np.random.randint(int(1e5))):\n",
    "        random_seed(seed)\n",
    "        self.state_dim = (2,)\n",
    "        self.max_x_dim = 15\n",
    "        self.max_y_dim = 15\n",
    "        self.action_dim = 4\n",
    "        self.obstacles_map = self.get_obstacles_map()\n",
    "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)] # right, left, down, up\n",
    "        self.min_x, self.max_x, self.min_y, self.max_y = 0, self.max_x_dim-1, 0, self.max_y_dim-1\n",
    "        self.goal_x, self.goal_y = 9, 9\n",
    "\n",
    "        self.current_state = None\n",
    "\n",
    "    def generate_state(self, coords):\n",
    "        return np.array(coords)\n",
    "\n",
    "    def info(self, key):\n",
    "        return\n",
    "\n",
    "    def reset(self):\n",
    "        while True:\n",
    "            rand_state = np.random.randint(low=0, high=max(self.max_x_dim, self.max_y_dim), size=2)\n",
    "            rx, ry = rand_state\n",
    "            if not int(self.obstacles_map[rx][ry]) and not (rx == self.goal_x and ry == self.goal_y):\n",
    "                self.current_state = rand_state[0], rand_state[1]\n",
    "                return self.generate_state(self.current_state)\n",
    "    \n",
    "    def step(self, a):\n",
    "        dx, dy = self.actions[a[0]]\n",
    "        x, y = self.current_state\n",
    "\n",
    "        nx = x + dx\n",
    "        ny = y + dy\n",
    "\n",
    "        nx, ny = min(max(nx, self.min_x), self.max_x), min(max(ny, self.min_y), self.max_y)\n",
    "\n",
    "        if not self.obstacles_map[nx][ny]:\n",
    "            x, y = nx, ny\n",
    "\n",
    "        self.current_state = x, y\n",
    "        if x == self.goal_x and y == self.goal_y:\n",
    "            return self.generate_state([x, y]), np.asarray(1.0), np.asarray(True), \"\"\n",
    "        else:\n",
    "            return self.generate_state([x, y]), np.asarray(0.0), np.asarray(False), \"\"\n",
    "\n",
    "    def get_visualization_segment(self):\n",
    "        state_coords = [[x, y] for x in range(self.max_x_dim)\n",
    "                       for y in range(self.max_y_dim) if not int(self.obstacles_map[x][y])]\n",
    "        states = [self.generate_state(coord) for coord in state_coords]\n",
    "        goal_coords = [[9, 9], [0, 0], [14, 0], [7, 14]]\n",
    "        goal_states = [self.generate_state(coord) for coord in goal_coords]\n",
    "        return np.array(states), np.array(state_coords), np.array(goal_states), np.array(goal_coords)\n",
    "\n",
    "    def get_obstacles_map(self):\n",
    "        _map = np.zeros([self.max_x_dim, self.max_y_dim])\n",
    "        _map[2, 0:6] = 1.0\n",
    "        _map[2, 8:] = 1.0\n",
    "        _map[3, 5] = 1.0\n",
    "        _map[4, 5] = 1.0\n",
    "        _map[5, 2:7] = 1.0\n",
    "        _map[5, 9:] = 1.0\n",
    "        _map[8, 2] = 1.0\n",
    "        _map[8, 5] = 1.0\n",
    "        _map[8, 8:] = 1.0\n",
    "        _map[9, 2] = 1.0\n",
    "        _map[9, 5] = 1.0\n",
    "        _map[9, 8] = 1.0\n",
    "        _map[10, 2] = 1.0\n",
    "        _map[10, 5] = 1.0\n",
    "        _map[10, 8] = 1.0\n",
    "        _map[11, 2:6] = 1.0\n",
    "        _map[11, 8:12] = 1.0\n",
    "        _map[12, 5] = 1.0\n",
    "        _map[13, 5] = 1.0\n",
    "        _map[14, 5] = 1.0\n",
    "\n",
    "        return _map\n",
    "\n",
    "    def get_useful(self, state=None):\n",
    "        if state:\n",
    "            return state\n",
    "        else:\n",
    "            return self.current_state\n",
    "\n",
    "class GridHardRGB(GridHardXY):\n",
    "    def __init__(self, seed=np.random.randint(int(1e5))):\n",
    "        super().__init__(seed)\n",
    "    \n",
    "        d = len(self.obstacles_map)\n",
    "        self.state_dim = (d, d, 3)\n",
    "\n",
    "        \"\"\"\n",
    "        # Gray-scale image\n",
    "            Walls are Red\n",
    "            Open spaces are Green\n",
    "            Agent is Blue\n",
    "        \"\"\"\n",
    "        self.rgb_template = np.zeros(self.state_dim)\n",
    "        for x in range(d):\n",
    "            for y in range(d):\n",
    "                if self.obstacles_map[x][y]:\n",
    "                    self.rgb_template[x][y][0] = 255.0\n",
    "                else:\n",
    "                    self.rgb_template[x][y][1] = 255.0\n",
    "\n",
    "    def generate_state(self, coords):\n",
    "        state = np.copy(self.rgb_template)\n",
    "        x, y = coords\n",
    "        assert state[x][y][1] == 255.0 and state[x][y][2] == 0.0\n",
    "\n",
    "        state[x][y][1] = 0.0    # setting the green color on\n",
    "        state[x][y][2] = 255.0  # turning the blue color on\n",
    "        return state\n",
    "\n",
    "    def get_features(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_useful(self, state=None):\n",
    "        blue = np.array([0., 0., 255.])\n",
    "        if state is None:\n",
    "            state = self.generate_state(self.current_state)\n",
    "        idx = np.where(np.all(state==blue, axis=2) == True)\n",
    "        coord = np.array([idx[0][0], idx[1][0]])\n",
    "        return coord\n",
    "\n",
    "class GridHardTabular(GridHardRGB):\n",
    "    def __init__(self, goal_state_idx, seed=np.random.randint(int(1e5))):\n",
    "        super().__init__(seed)\n",
    "        self.nos = (self.max_x_dim*self.max_y_dim)-int(np.sum(self.obstacles_map))\n",
    "        self.state_idx_map = [(i, j) for i in range(self.max_x_dim)\\\n",
    "                              for j in range(self.max_y_dim) if not self.obstacles_map[i, j]]\n",
    "        self.idx_state_map = np.zeros((self.max_x_dim, self.max_y_dim)).astype(np.int)\n",
    "        \n",
    "        for i in range(self.nos):\n",
    "            x, y = self.state_idx_map[i]\n",
    "            self.idx_state_map[x][y] = i\n",
    "            \n",
    "        self.goal_x, self.goal_y = self.state_idx_map[goal_state_idx]\n",
    "        self.goal_state_idx = goal_state_idx\n",
    "        \n",
    "    def get_the_model(self):\n",
    "        \n",
    "        transition_matrix = np.zeros((self.nos, self.action_dim, self.nos))\n",
    "        reward_model = np.zeros((self.nos, self.action_dim))\n",
    "        \n",
    "        for state_idx in range(self.nos):\n",
    "            for a in range(self.action_dim):\n",
    "                \n",
    "                dx, dy = self.actions[a]\n",
    "                x, y = self.state_idx_map[state_idx]\n",
    "\n",
    "                nx = x + dx\n",
    "                ny = y + dy\n",
    "\n",
    "                nx, ny = min(max(nx, self.min_x), self.max_x), min(max(ny, self.min_y), self.max_y)\n",
    "\n",
    "                if not self.obstacles_map[nx][ny]:\n",
    "                    x, y = nx, ny\n",
    "                \n",
    "                next_state_idx = self.idx_state_map[x, y]\n",
    "                transition_matrix[state_idx, a, next_state_idx] = 1.\n",
    "                if (x, y) == (self.goal_x, self.goal_y):\n",
    "                    if state_idx != self.goal_state_idx:\n",
    "                        reward_model[state_idx, a] = 1.0\n",
    "                    \n",
    "        return transition_matrix, reward_model\n",
    "\n",
    "    \n",
    "class GridHardTabularRaw(GridHardTabular):\n",
    "    \"\"\"\n",
    "    This is the environment with arbitrary number of rows and columns\n",
    "    \"\"\"\n",
    "    def __init__(self, goal_state_idx, seed=np.random.randint(int(1e5)), max_x_dim=15, max_y_dim=15):\n",
    "        super().__init__(goal_state_idx, seed)\n",
    "\n",
    "        self.max_x_dim = max_x_dim\n",
    "        self.max_y_dim = max_y_dim\n",
    "\n",
    "        self.nos = (self.max_x_dim*self.max_y_dim)-int(np.sum(self.obstacles_map))\n",
    "        self.state_idx_map = [(i, j) for i in range(self.max_x_dim)\\\n",
    "                              for j in range(self.max_y_dim) if not self.obstacles_map[i, j]]\n",
    "        self.idx_state_map = np.zeros((self.max_x_dim, self.max_y_dim)).astype(np.int)\n",
    "        \n",
    "        for i in range(self.nos):\n",
    "            x, y = self.state_idx_map[i]\n",
    "            self.idx_state_map[x][y] = i\n",
    "            \n",
    "        self.goal_x, self.goal_y = self.state_idx_map[goal_state_idx]\n",
    "        \n",
    "        self.min_x, self.max_x, self.min_y, self.max_y = 0, self.max_x_dim-1, 0, self.max_y_dim-1\n",
    "            \n",
    "        \n",
    "    def get_obstacles_map(self):\n",
    "        _map = np.zeros([self.max_x_dim, self.max_y_dim])\n",
    "        _map[1, 1] = 1.\n",
    "        return _map\n",
    "    \n",
    "    \n",
    "class GridHardTabularRawVI(GridHardTabularRaw):\n",
    "    \"\"\"\n",
    "    This is the implementation where the transitions from goal states will results in any other state except the goal state arbitrary.\n",
    "    \"\"\"\n",
    "    def __init__(self, goal_state_idx, seed=np.random.randint(int(1e5)),\n",
    "                 max_x_dim=15, max_y_dim=15):\n",
    "        super().__init__(goal_state_idx, seed, max_x_dim, max_y_dim)\n",
    "  \n",
    "    def get_the_model(self):\n",
    "        \n",
    "        transition_matrix = np.zeros((self.nos, self.action_dim, self.nos))\n",
    "        reward_model = np.zeros((self.nos, self.action_dim))\n",
    "        \n",
    "        for state_idx in range(self.nos):\n",
    "            for a in range(self.action_dim):\n",
    "                \n",
    "                dx, dy = self.actions[a]\n",
    "                x, y = self.state_idx_map[state_idx]\n",
    "                if (x, y) == (self.goal_x, self.goal_y):\n",
    "                    transition_matrix[state_idx, a, :] = 1./(self.nos-1.)\n",
    "                    transition_matrix[state_idx, a, state_idx] = 0.\n",
    "                else:\n",
    "                    nx = x + dx\n",
    "                    ny = y + dy\n",
    "\n",
    "                    nx, ny = min(max(nx, self.min_x), self.max_x), min(max(ny, self.min_y), self.max_y)\n",
    "\n",
    "                    if not self.obstacles_map[nx][ny]:\n",
    "                        x, y = nx, ny\n",
    "\n",
    "                    next_state_idx = self.idx_state_map[x, y]\n",
    "                    transition_matrix[state_idx, a, next_state_idx] = 1.\n",
    "                    if (x, y) == (self.goal_x, self.goal_y):\n",
    "                        if state_idx != self.goal_state_idx:\n",
    "                            reward_model[state_idx, a] = 1.0\n",
    "                            \n",
    "        return transition_matrix, reward_model\n",
    "                    \n",
    "    def get_obstacles_map(self):\n",
    "        _map = np.zeros([self.max_x_dim, self.max_y_dim])\n",
    "        return _map\n",
    "\n",
    "    \n",
    "\n",
    "class GridHardTabularRawVII(GridHardTabularRaw):\n",
    "    \"\"\"\n",
    "    This is the implementation where the transitions from goal states will results in the initial state.\n",
    "    \"\"\"\n",
    "    def __init__(self, goal_state_idx, start_state_idx, seed=np.random.randint(int(1e5)),\n",
    "                 max_x_dim=15, max_y_dim=15):\n",
    "        super().__init__(goal_state_idx, seed, max_x_dim, max_y_dim)\n",
    "        self.start_state_idx = start_state_idx\n",
    "  \n",
    "    def get_the_model(self):\n",
    "        \n",
    "        transition_matrix = np.zeros((self.nos, self.action_dim, self.nos))\n",
    "        reward_model = np.zeros((self.nos, self.action_dim))\n",
    "        \n",
    "        for state_idx in range(self.nos):\n",
    "            for a in range(self.action_dim):\n",
    "                \n",
    "                dx, dy = self.actions[a]\n",
    "                x, y = self.state_idx_map[state_idx]\n",
    "                if (x, y) == (self.goal_x, self.goal_y):\n",
    "                    transition_matrix[state_idx, a, self.start_state_idx] = 1.\n",
    "                    transition_matrix[state_idx, a, state_idx] = 0.\n",
    "                else:\n",
    "                    nx = x + dx\n",
    "                    ny = y + dy\n",
    "\n",
    "                    nx, ny = min(max(nx, self.min_x), self.max_x), min(max(ny, self.min_y), self.max_y)\n",
    "\n",
    "                    if not self.obstacles_map[nx][ny]:\n",
    "                        x, y = nx, ny\n",
    "\n",
    "                    next_state_idx = self.idx_state_map[x, y]\n",
    "                    transition_matrix[state_idx, a, next_state_idx] = 1.\n",
    "                    if (x, y) == (self.goal_x, self.goal_y):\n",
    "                        if state_idx != self.goal_state_idx:\n",
    "                            reward_model[state_idx, a] = 1.0\n",
    "        return transition_matrix, reward_model\n",
    "                    \n",
    "    def get_obstacles_map(self):\n",
    "        _map = np.zeros([self.max_x_dim, self.max_y_dim])\n",
    "        return _map\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c01c5f",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "80502b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of states:  4\n",
      "Goal 0 Converged\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nos = 4\n",
    "optimal_policies = []\n",
    "max_x_dim = 2\n",
    "max_y_dim = 2\n",
    "start_state = 1\n",
    "for goal_state in range(nos)[0:1]:\n",
    "    env = GridHardTabularRawVI(goal_state, 0, max_x_dim, max_y_dim)\n",
    "    env.reset()\n",
    "\n",
    "    P, R = env.get_the_model()\n",
    "    nos = env.nos  # no of states\n",
    "    noa = env.action_dim\n",
    "    print('The number of states: ', env.nos)\n",
    "    \n",
    "    delta = 0.00\n",
    "    gamma = 0.99\n",
    "    max_diff = 0\n",
    "    \n",
    "    V = np.zeros(nos)\n",
    "    for time in range(0, 10):\n",
    "        Vnew = np.zeros(nos)\n",
    "        for i in range(nos):\n",
    "            for a in range(noa):\n",
    "                cur_val = 0\n",
    "                for j in np.nonzero(P[i][a])[0]:\n",
    "                    cur_val += P[i][a][j]*V[j]\n",
    "\n",
    "                if goal_state == i:\n",
    "                    cur_val *= 0.\n",
    "                else:\n",
    "                    cur_val *= gamma\n",
    "                cur_val += R[i][a]\n",
    "\n",
    "                Vnew[i] = max(Vnew[i], cur_val)\n",
    "        max_diff = 0\n",
    "        for i in range(nos):\n",
    "            max_diff = max(max_diff, abs(V[i]-Vnew[i]))\n",
    "            \n",
    "        V = Vnew\n",
    "        \n",
    "        if(max_diff == delta):\n",
    "            print('Goal {} Converged'.format(goal_state))\n",
    "            break\n",
    "\n",
    "    # one final iteration to determine the policy\n",
    "    Vmax = np.zeros(nos)\n",
    "    optimal_policy = np.zeros((nos, noa))\n",
    "    optimal_actions = [[] for _ in range(nos)]\n",
    "    for i in range(nos):\n",
    "        for a in range(noa):\n",
    "            cur_val = 0\n",
    "            for j in range(nos):\n",
    "                cur_val += P[i][a][j]*V[j]\n",
    "            if goal_state == i:\n",
    "                cur_val *= 0.\n",
    "            else:\n",
    "                cur_val *= gamma\n",
    "            cur_val += R[i][a]\n",
    "            if(Vmax[i] < cur_val):\n",
    "                Vmax[i] = max(Vmax[i], cur_val)\n",
    "\n",
    "        for a in range(noa):\n",
    "            cur_val = 0\n",
    "            for j in range(nos):\n",
    "                cur_val += P[i][a][j]*V[j]\n",
    "            if goal_state == i:\n",
    "                cur_val *= 0\n",
    "            else:\n",
    "                cur_val *= gamma\n",
    "            cur_val += R[i][a]\n",
    "            if(Vmax[i] == cur_val):\n",
    "                optimal_actions[i].append(a)\n",
    "                \n",
    "        for a in optimal_actions[i]:\n",
    "            optimal_policy[i, a] = 1.\n",
    "        optimal_policy[i, :] = optimal_policy[i, :]/np.sum(optimal_policy[i, :])\n",
    "    optimal_policies.append(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ead1065",
   "metadata": {},
   "source": [
    "## Computing the policy models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5c9c9d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.00\n",
    "gamma = 0.99\n",
    "max_diff = 0\n",
    "\n",
    "policy_models = []\n",
    "for goal_state in range(nos)[0:1]:\n",
    "    env = GridHardTabularRawVI(goal_state, 0, max_x_dim, max_y_dim)\n",
    "    env.reset()\n",
    "\n",
    "    nos = env.nos  # no of states\n",
    "    policy_model = np.zeros((nos, nos))\n",
    "\n",
    "#     print(nos)\n",
    "    noa = env.action_dim\n",
    "    P, R = env.get_the_model()\n",
    "#     print(R.shape)\n",
    "    for i in range(nos):\n",
    "        for j in range(nos):\n",
    "#             print('i, j: ', i, j)\n",
    "            policy_model[i][j] = np.dot(P[i, :, j], optimal_policies[goal_state][i, :])\n",
    "            \n",
    "    policy_models.append(policy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9042b69",
   "metadata": {},
   "source": [
    "## Caculating the SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b105d532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[43.16484655, 21.29537704, 21.29537704, 14.24439936],\n",
       "       [42.73319808, 22.08242327, 21.08242327, 14.10195537],\n",
       "       [42.73319808, 21.08242327, 22.08242327, 14.10195537],\n",
       "       [42.3058661 , 21.36659904, 21.36659904, 14.96093581]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(np.identity(4) - gamma * policy_models[0])[:, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6121a62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(69)\n",
    "# test_goals = []\n",
    "# for c in np.random.choice(range(225), 30, replace=False):\n",
    "#     test_goals.append((int(c//15), c%15))\n",
    "# print(test_goals)\n",
    "# state_idx_map = [(i, j) for i in range(14) for j in range(14) if not self.obstacles_map[i, j]]\n",
    "# print(state_idx_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c99afafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env = GridHardTabularRaw(0, 0, 2, 1)\n",
    "# env.nos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db7442a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa738284f10>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# env = GridHardTabular(0, 0)\n",
    "\n",
    "# img = env.reset()\n",
    "\n",
    "# plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
