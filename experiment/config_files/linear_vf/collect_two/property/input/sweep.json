{    "config_class": "DQNAgentConfig",
    "fixed_parameters": {
        "exp_name": "linear_vf/collect_two/property/input/sweep",
        "env_name": "CollectTwoColorRGB",
        "linear_probing_parent": "DQNAgent",
        "discount": 0.99,
        "state_norm_coef": 255.0,

        "epsilon": 1.0,

        "rep_config": {
            "rep_type": "identity",
            "in_dim": [15, 15, 3],
            "out_dim": 675,
            "train_rep": false,
            "load_params": false
        },

        "val_fn_config": {
            "val_fn_type": "linear"
        },

        "replay": true,
        "memory_size": 10000,
        "batch_size": 32,

        "optimizer_type": "Adam",
        "use_target_network": true,
        "target_network_update_freq": 1024,

        "max_steps": 1000000,
        "log_interval": 100,
        "eval_interval": 100,
        "eval_episodes": 5,
        "save_interval": 0,
        "timeout": 500,
        "stats_queue_size": 100,

        "distance_path": {"current": "linear_vf/gridhard/random_dqn/distance_current_states.npy", "action": "linear_vf/gridhard/random_dqn/distance_actions.npy", "next": "linear_vf/gridhard/random_dqn/distance_next_states.npy", "reward": "linear_vf/gridhard/random_dqn/distance_rewards.npy", "terminal": "linear_vf/gridhard/random_dqn/distance_terminals.npy"},
        "linearprob_path": {"train": "linear_vf/gridhard/random_dqn/lip_sampled_states.npy", "test": "linear_vf/gridhard/random_dqn/retaining_test_states.npy"},
        "linearprob_tasks": [
            {"task": "xy", "loss":  "mse", "truth_start":  0, "truth_end":  2},
            {"task": "color", "loss":  "cross-entropy", "truth_start":  2, "truth_end":  3, "num_class":  4},
            {"task": "count", "loss":  "mse", "truth_start":  14, "truth_end":  16}
        ],
        "coord_dim": 2,
        "retain": "current",
        "linear_max_steps": 50000000
    },
    "sweep_parameters": {
        "learning_rate": [0.1, 0.01, 0.001, 0.0001]
    }
}
